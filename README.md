# ML-R
## 1. Naive Bayesian(朴素贝叶斯)
### 一句话概括
P(A∩B)=P(A)*P(B|A)=P(B)*P(A|B)
所以有：P(A|B)=P(B|A)*P(A)/P(B)
对于给出的待分类项，求解在此项出现的条件下各个目标类别出现的概率，哪个最大，就认为此待分类项属于哪个类别
### 工作原理
*  假设现在有样本x=(a1,a2,a3,…an)这个待分类项(并认为x里面的特征独立)
*  假设现在有分类目标Y={y1,y2,y3,y4..yn}
*  那么max(P(y1|x),P(y2|x),P(y3|x)..P(yn|x))中的最大者就是最终的分类类别
*  而P(yi|x)=p(x|yi)*P(yi)/P(x)
*  因为x对于每个分类目标来说都一样，所以就是求max(P(x|yi)*p(yi))
*  P(x|yi)*p(yi)=p(yi)*PI(P(ai|yi)) (PI表示连乘)
*  而具体的p(ai|yi)和p(yi)都是能从训练样本中统计出来;
   p(ai|yi)表示该类别下该特征出现的概率;
   p(yi)表示全部类别中这个这个类别出现的概率;
### 工作流程
* **准备阶段**：确定特征属性，并对每个特征属性进行适当划分，然后由人工对一部分待分类项进行分类，形成训练样本
* **训练阶段**：计算每个类别在训练样本中的出现频率及每个特征属性划分对每个类别的条件概率估计
* **应用阶段**：使用分类器进行分类，输入是分类器和待分类样本，输出是样本属于的分类类别
### 优缺点
* **优点**：对小规模的数据表现很好，适合多分类任务，适合增量式训练
* **缺点**：对输入数据的表达形式很敏感（离散、连续，值极大极小之类的）
## 2. Knn(K-Nearest neighbors,K近邻)
### 一句话概括
给一个训练数据集和一个新的实例，在训练数据集中找出与这个新实例最近的k个训练实例，然后统计最近的k个训练实例中所属类别计数最多的那个类，就是新实例的类
### 三个要素
* K值的选择
1. 交叉验证法选择最优K值
2. K值一般低于训练样本上的平方根
* 距离度量的计算方式
1. 距离有多种计算方式（欧式、马氏、切比雪夫等），但最常用的还是欧式距离
* 分类决策规则
1. 多数表决制；K个邻居当中出现次数最多的类即为分类结果
2. 加权投票制；多数表决制只考虑K个邻居的分类情况，而未考虑K个邻居与预测点的距离；加权投票制就是考虑了距离因素，将待分类点与每个邻居的距离乘以权重；较常用的方法是选取距离的倒数为对应权重，距离越小权重越大，然后将权重累加，最后权重累加值最高的类别即为分类结果
### 优缺点
* **优点**：
 1. 原理简单，无需训练，容易理解实现
 2. 适合对稀有事件进行分类（？）
 3. 特别适合多分类问题（效果一般比svm好）
* **缺点**：
 1. 懒惰算法，计算量大，速度慢
 2. 不同类样本不均匀时，分类结果容易有偏
## 3. Logistic Regression
